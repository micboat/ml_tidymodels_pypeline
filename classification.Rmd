# Description

encoding: utf-8
press "ctrl + shift + o (windows)" to show document outline
press "alt + o" to collapse all
press "alt + shift + o" to expand all

# Load Librarys

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(naniar)     # vis_miss
library(janitor)    # rename colnames to under score
library(furrr)      # parallel
library(rlang)      # expression
library(earth)      # modeling mars
library(xgboost)    # modeling xgboost
library(lightgbm)   # modeling lgbm
library(Matrix)     # modeling lgbm
library(tictoc)     # time check
library(patchwork)  # plot
library(AUC)        # roc auc
library(caret)      # confusion matrix
source("func.R")
plan(multiprocess)
```

# Read Data 

**depends on project**

```{r}
library(tune)
library(kernlab)

# Load data
load(url("http://bit.ly/seg-data"))

segmentationData <-
  segmentationData %>%
  select(-Case, -Cell, -contains("Centroid"))
data <- segmentationData
glimpse(data)
```

# EDA

**depends on project**
**or separate the scripts**

```{r}
# vis_miss(data)
data$Class %>% table %>% prop.table()
```



# Split the Data and CV Data

```{r}
set.seed(7777)
splits <- data %>% initial_split(prop = 0.8, strata = Class)

train <- training(splits)
test <- testing(splits)

data_cv <- train %>% vfold_cv(v = 5) 
```

# Recipes (Preprocessing)

```{r}
# set target_var
target_var <- sym("Class")
# set positve and negatibe
positive <- "WS"
negative <- "PS"
# set estimate type
mode_reg_or_clas <- "classification"
# set formura
formula <- expr(formula(!!target_var ~ .))
# set recipes
rec <-
  data %>%
  recipe(formula = formula) %>%
  step_YeoJohnson(-!!target_var) %>%
  step_normalize(-!!target_var) %>%
  step_pca(-!!target_var, num_comp = 15) %>% 
  step_downsample(!!target_var) 
 
rec_preped <- prep(rec)
rec_preped %>% juice() %>% pull(Class) %>% table() %>% prop.table()
```

# No Parameter Tuning

## Prepare the Model

```{r}
model_logit <- logistic_reg() %>% set_engine("glm") # Logistic regression
model_tree <- decision_tree() %>% set_engine("rpart") # Decision tree
model_svm <-   svm_rbf()      %>% set_engine("kernlab") # svm
model_rf <-    rand_forest()  %>% set_engine("ranger") # random forest
model_mars <-  mars()         %>% set_engine("earth") # mars model
model_xgb <-   boost_tree()   %>% set_engine("xgboost") #boosted trees
model_keras <- mlp()          %>% set_engine("keras") #neural nets

# Create Model List
model_list_no_tuning <-
  list(
    logit = model_logit,
    tree = model_tree,
    svm = model_svm,
    rf = model_rf,
    mars = model_mars,
    xgb = model_xgb
  ) %>%
  map(set_mode, mode_reg_or_clas)
```

## Solo model 

### Modeling all

```{r}
fits_solo_models <-
  model_list_no_tuning %>%
  map(fit,
      formula = formula(formula),
      data = juice(rec_preped))
# model_list_no_tuning$logit %>% fit(formula(formula), juice(rec_preped))

res_solo_models <-
  fits_solo_models %>%
  map(predict,
      bake(rec_preped, train), type = "prob") %>%
  map(bind_cols, bake(rec_preped, train))

res_solo_models
```

### lightGBM

```{r}
# lgb_dataset <-
#   create_lgb_dataset(target_var = target_var,
#                      rec_preped = rec_preped,
#                      test = test)
# params <-
#   list(
#     eta = 0.05, 
#     max_depth = 6, 
#     num_leaves = 31,
#     colsample_bytree = 0.3,
#     objective = "binary", 
#     metric = "auc"
#     )
```

```{r}
# tic()
# fit_lgb <-
#   lgb.train(params = params,
#             data = lgb_dataset$lgb_data)
# toc()
# 
# res_lgb <-
#   test %>%
#   bind_cols(fit = predict(fit_lgb, lgb_dataset$lgb_test_mat)) 
# 
# res_lgb %>% select(target_var, fit)

```


### ROC Curve and AUC

```{r}
pred_positive <- sym(paste0(".pred_", positive))

roc_curve_and_auc(res_solo_models)
```

### Adjust Threshold and Predict

```{r}
adjusted_thre_solo_models <-
  res_solo_models %>% future_map(threshold_f1score, lo = 0.1, hi = 0.83)
adjusted_thre_solo_models

res_solo_models <- 
  predict_by_adjusted_threshold(res_solo_models, adjusted_thre_solo_models)
```


### Confusion metrics 

```{r}
confusion_matrix_all <- confusion_matrix_info(res_solo_models)
confusion_matrix_all
```


**maybe return to EDA and preprocessing**

## Cross Validation

### Modeling by No Tuning CV

```{r}
fits_no_tuning <- fit_cv_no_tuning(model_list_no_tuning)
fits_no_tuning
```

### Result no tuning cv

```{r}
# metric by threshold 0.5 default
fits_no_tuning$fits %>%
  map(collect_metrics) %>% 
  bind_rows() %>% 
  mutate(model = rep(names(fits_no_tuning$fits), each=2)) %>% 
  arrange(.metric, desc(mean))
```

### ROC Curve for folds

```{r}
p <- fits_no_tuning$fits %>% 
  map(collect_predictions) %>% 
  map(group_by,id) %>% 
  map(roc_curve, !!target_var, !!pred_positive) %>%
  map(autoplot) %>% 
  map2(names(.), wrapper_add_title)

p
```


# Tuning 

## Prepare the Models (tuning)

```{r}
size = 20
rec <-
  data %>%
  recipe(formula = formula) %>%
  step_YeoJohnson(-!!target_var) %>%
  step_normalize(-!!target_var) %>%
  step_pca(-!!target_var ,num_comp = 15) %>% 
  step_downsample(!!target_var) 

# Logistic regression
tune_logit <- model_logit %>% 
  set_engine("glmnet") %>% 
  update(penalty = tune(), 
         mixture = tune())

grid_logit <- tune_logit %>%
  parameters() %>%
  grid_max_entropy(size = size)

# decision tree
tune_tree <- model_tree %>% 
  update(tree_depth = tune(),
         cost_complexity = tune(),
         min_n = tune())

grid_tree <- tune_tree %>% 
  parameters() %>% 
  finalize(select(data, -target_var)) %>% 
  grid_max_entropy(size = size)

# svm
tune_svm <- model_svm %>% 
  update(cost = tune(),
         rbf_sigma = tune())

grid_svm <- tune_svm %>% 
  parameters() %>% 
  grid_max_entropy(size = size)

# random forest
tune_rf <- model_rf %>%
  update(mtry = tune(),
         trees = tune())

grid_rf <-
  tune_rf %>%
  parameters() %>%
  finalize(select(data, -target_var)) %>%
  grid_max_entropy(size = size)

# mars model
tune_mars <- model_mars %>%
  update(
    num_terms = tune(),
    prod_degree = 2,
    prune_method = tune()
  )

grid_mars <-
  tune_mars %>%
  parameters() %>%
  grid_max_entropy(size = size)

#boosted trees
tune_xgb <- model_xgb %>%
  update(
    mtry = tune(),
    tree = tune(),
    min_n = tune(),
    learn_rate = tune(),
    tree_depth = tune()
  )

grid_xgb <- tune_xgb %>%
  parameters() %>%
  finalize(select(data, -target_var)) %>%
  grid_max_entropy(size = size)

#neural nets
tune_keras <- model_keras %>%
  update(hidden_units = tune(),
         penalty = tune(),
         activation = "relu")

grid_keras <-
  tune_keras %>%
  parameters() %>%
  grid_max_entropy(size = size)

```


## Create Model and Grid List  (tuning)

```{r}
model_list_tuning <-
  list(
    logit = tune_logit,
    tree = tune_tree,
    svm = tune_svm,
    rf = tune_rf,
    mars = tune_mars,
    xgb = tune_xgb
  ) %>% 
  map(set_mode,mode_reg_or_clas)
grid_list <-
  list(
    logit = grid_logit,
    tree = grid_tree,
    svm = grid_svm,
    rf = grid_rf,
    mars = grid_mars,
    xgb = grid_xgb
  )
model_list_tuning
grid_list
```

## Modeling by Tuning CV

```{r}
plan(sequential)
plan(multiprocess)
fits_tuning <- fit_cv_tuning(model_list_tuning, grid_list)
fits_tuning
```

```{r}
# set.seed(1291)
# search_res <- 
#   tune_bayes(
#     svm_wflow, 
#     resamples = folds,
#     # To use non-default parameter ranges
#     param_info = svm_set,
#     # Generate five at semi-random to start
#     initial = 3,
#     iter = 30,
#     # How to measure performance?
#     metrics = metric_set(roc_auc),
#     control = control_bayes(no_improve = 20, verbose = TRUE)
#   )
```

## Result tuning cv

```{r}
fits_tuning$fit %>% map(collect_metrics) %>%  map(arrange,.metric, desc(mean))
```

```{r}
# fits_tuning$fit %>% map(collect_predictions)
```

```{r}
fits_tuning$fit %>% map(show_best, metric = "roc_auc", maximize = TRUE, n = 3)
```


# Refit by Best Params

## Update Models to Best Params

```{r}
best_param_list <-
  fits_tuning$fit %>% 
    map(show_best, metric = "roc_auc", maximize = TRUE, n = 1) %>% 
    map(select, -c(".metric", ".estimator", "mean", "n", "std_err"))

best_tune_logit <- tune_logit %>% update(best_param_list$logit)
best_tune_tree <- tune_tree %>% update(best_param_list$tree)
best_tune_svm <- tune_svm %>% update(best_param_list$svm)
best_tune_rf <- tune_rf %>% update(best_param_list$rf)
best_tune_mars <- tune_mars %>% update(best_param_list$mars)
best_tune_xgb <- tune_xgb %>% update(best_param_list$xgb)

```

## Create Model List  (best params)

```{r}
model_list_best_params <-
  list(
    logit = best_tune_logit,
    tree = best_tune_tree,
    svm = best_tune_svm,
    rf = best_tune_rf,
    mars = best_tune_mars,
    xgb = best_tune_xgb
  ) %>% 
  map(set_mode, mode_reg_or_clas)
```

## Modeling and Predict by Best Params 

```{r}
fits_best_models <-
  model_list_best_params %>%
  map(fit,
      formula = formula(formula),
      data = juice(rec_preped))
# model_list_no_tuning$logit %>% fit(formula(formula), juice(rec_preped))

res_best_models <-
  fits_best_models %>%
  map(predict,
      bake(rec_preped, train), type = "prob") %>%
  map(bind_cols, bake(rec_preped, train))

res_best_models
```

## ROC Curve and AUC

```{r}
pred_positive <- sym(paste0(".pred_", positive))

roc_curve_and_auc(res_best_models)
```

## Adjust Threshold and Predict

```{r}
adjusted_thre_best_models <-
  res_best_models %>% future_map(threshold_f1score, lo = 0.1, hi = 0.83)
adjusted_thre_best_models

res_best_models <- 
  predict_by_adjusted_threshold(res_solo_models, adjusted_thre_solo_models)
```


## Confusion metrics 

```{r}
confusion_matrix_all <- confusion_matrix_info(res_best_models)

confusion_matrix_all$confusion_matrix_all
confusion_matrix_all$heatmap %>% wrap_plots()
confusion_matrix_all$mosaic %>% wrap_plots()
```


# Predict Test

```{r}
baked_test <- test %>% bake(rec_preped, new_data = .)

res_test <- 
  fits_best_models %>%
  map(predict,
    new_data = baked_test %>% select(-target_var), type = "prob") %>% 
  map(bind_cols, baked_test)

res_test
preds_d <- preds_list %>% bind_cols() %>% select(contains("WS"))
colnames(preds_d) <- paste0("pred_WS_", names(preds_list))

preds_d <- baked_test %>% bind_cols(preds_d) 
preds_d

```

## ROC Curve and AUC

```{r}
pred_positive <- sym(paste0(".pred_", positive))

roc_curve_and_auc(res_test)
```

## Adjust Threshold and Predict

```{r}
res_test <- 
  predict_by_adjusted_threshold(res_test, adjusted_thre_solo_models)
```

## Result

```{r}
res_test %>% 
  map(select,!!target_var, contains("pred"))
```


## Confusion metrics 

```{r}
library(patchwork)
confusion_matrix_all <- confusion_matrix_info(res_test)

confusion_matrix_all$confusion_matrix_all
confusion_matrix_all$heatmap %>% 
  wrap_plots() +
  plot_annotation(title = "Confusion Matrix (heatmap)")
confusion_matrix_all$mosaic %>% 
  wrap_plots() +
  plot_annotation(title = "Confusion Matrix (mosaic)",)

```


# Final test (submit)


```{r}
# saveRDS(fits_tuning, "fits_tuning.rds")
# saveRDS(fits_best_params, "fits_best_params.rds")
```


