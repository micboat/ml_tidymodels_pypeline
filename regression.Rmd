# Description

encoding: utf-8
press "ctrl + shift + o (windows)" to show document outline
press "alt + o" to collapse all
press "alt + shift + o" to expand all

# Load Librarys

```{r}
library(tidyverse)
library(tidymodels)
library(naniar)     # vis_miss
library(janitor)    # rename colnames to under score
library(furrr)      # parallel
library(rlang)      # expression
library(earth)      # modeling mars
library(xgboost)    # modeling xgboost
library(lightgbm)   # modeling lgbm
library(Matrix)     # modeling lgbm
library(tictoc)     # time check
library(patchwork)  # plot
source("func.R")
```

# Read Data 

**depends on project**

```{r}
data <- diamonds
glimpse(diamonds)
```

# EDA

**depends on project**
**or separate the scripts**

```{r}
# vis_miss(data)
```



# Split the Data and CV Data

```{r}
set.seed(7777)
splits <- data %>% initial_split(prop = 0.8)

train <- training(splits)
test <- testing(splits)

data_cv <- train %>% vfold_cv(v = 5) 
  
```

# Recipes (Preprocessing)

```{r}
# set target_var
target_var <- sym("carat")
# set estimate type
mode_reg_or_clas <- "regression"
# set formura
formula <- expr(formula(!!target_var ~ .))
# set recipes
rec <-
  data %>%
  recipe(formula = formula) %>%
  step_ordinalscore(cut, color, clarity)

rec_preped <-
  rec %>%
  prep()
```


# No Parameter Tuning

## Prepare the Model

```{r}

# linear regression
model_lm <-
  linear_reg() %>% 
  set_engine("lm") %>%
  set_mode(mode_reg_or_clas) # %>% translate()

wflow_lm <-
  workflow() %>% 
  add_recipe(rec) %>%
  add_model(model_lm) 

# random forest
model_rf <-
  rand_forest() %>% 
  set_engine("ranger") %>%
  set_mode(mode_reg_or_clas) 

wflow_rf <- 
  workflow() %>% 
  add_recipe(rec) %>% 
  add_model(model_rf)

# mars model
model_mars <-
  mars() %>% 
  set_engine("earth") %>% 
  set_mode(mode_reg_or_clas)

wflow_mars <-
  workflow() %>% 
  add_recipe(rec) %>%
  add_model(model_mars)

#boosted trees
model_xgb <-
  boost_tree() %>%
  set_engine("xgboost") %>% 
  set_mode(mode_reg_or_clas)

wflow_xgb <- 
  workflow() %>% 
  add_recipe(rec) %>%
  add_model(model_xgb)

#neural nets
model_keras <-
  mlp() %>%
  set_engine("keras") %>% 
  set_mode(mode_reg_or_clas)

wflow_keras <-
  workflow() %>%
  add_recipe(rec) %>%
  add_model(model_keras)
```

## Soro model

### xgboost

```{r}
# fitting
fit_xgb <- fit(wflow_xgb, train)

# predict test, and pred to the column
res_xgb <-
  test %>%
  bake(rec_preped, new_data = .) %>%
  tidypredict::tidypredict_to_column(df = ., fit_xgb$fit$fit$fit)
```


### lightGBM

```{r}
lgb_dataset <-
  create_lgb_dataset(target_var = target_var,
                     rec_preped = rec_preped,
                     test = test)
params <-
  list(objective = "regression",
       metric = "rmse")

```

```{r}
tic()
fit_lgb <-
  lgb.train(params = params,
            data = lgb_dataset$lgb_data)
toc()
```

#### feature importance (xgb)

```{r}
my_importance_plot(fit_xgb, res_xgb)
```

     
#### feature importance (lgb)

```{r}
res_lgb <-
  test %>%
  bind_cols(fit = predict(fit_lgb, lgb_dataset$lgb_test_mat))

my_importance_plot(fit_lgb, res_lgb)
```

### metric (xgb and lgb)

```{r}
bind_rows(
  res_xgb %>% metrics(carat, fit) %>% mutate(boost = "xgb"),
  res_lgb %>% metrics(carat, fit) %>% mutate(boost = "lgb")
) %>%
  arrange(.metric) %>% 
  select(-.estimator)
```

**maybe return to EDA and preprocessing**

## Cross Validation

### Create Wflow List

```{r}
wflow_list_no_tuning <-
  list(lm = wflow_lm, rf = wflow_rf)
```

### Modeling by No Tuning CV

```{r}
# plan(multiprocess)
# fits_no_tuning <- fit_cv_no_tuning(wflow_list_no_tuning)
# fits_no_tuning
```

# Tuning 

## Prepare the Models (tuning)

```{r}
# linear regression (lasso)
tune_lm <-
  linear_reg(penalty = tune(),
             mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode(mode_reg_or_clas)

# Hyperparameter grid
grid_lm <-
  tune_lm %>%
  parameters() %>%
  grid_max_entropy(size = 5)

# Workflow bundling every step
wflow_lm <-
  workflow() %>%
  add_recipe(rec) %>%
  add_model(tune_lm)

# random forest
tune_rf <- model_rf %>%
  update(mtry = tune(),
         trees = tune())

grid_rf <-
  tune_rf %>%
  parameters() %>%
  finalize(select(data, -target_var)) %>%
  grid_max_entropy(size = 5)

wflow_rf <-
  workflow() %>%
  add_recipe(rec) %>%
  add_model(tune_rf)

# mars model
tune_mars <- model_mars %>%
  update(
    num_terms = tune(),
    prod_degree = 2,
    prune_method = tune()
  )

grid_mars <-
  tune_mars %>%
  parameters() %>%
  grid_max_entropy(size = 5)

wflow_mars <-
  workflow() %>%
  add_recipe(rec) %>%
  add_model(tune_mars)

#boosted trees
tune_xgb <- model_xgb %>%
  update(
    mtry = tune(),
    tree = tune(),
    min_n = tune(),
    learn_rate = tune(),
    tree_depth = tune()
  )

grid_xgb <- tune_xgb %>%
  parameters() %>%
  finalize(select(data, -target_var)) %>%
  grid_max_entropy(size = 5)

wflow_xgb <-
  workflow() %>%
  add_recipe(rec) %>%
  add_model(tune_xgb)

#neural nets
tune_keras <- model_keras %>%
  update(hidden_units = tune(),
         penalty = tune(),
         activation = "relu")

grid_keras <-
  tune_keras %>%
  parameters() %>%
  grid_max_entropy(size = 5)

wflow_keras <-
  workflow() %>%
  add_recipe(rec) %>%
  add_model(tune_keras)
```


## Create Wflow and Grid List  (tuning)

```{r}
wflow_list_tuning <-
  list(lm = wflow_lm, rf = wflow_rf, mars = wflow_mars, xgb = wflow_xgb)
grid_list <-
  list(lm = grid_lm, rf = grid_rf, mars = grid_mars, xgb = grid_xgb)
wflow_list_tuning
grid_list
```

## Modeling by Tuning CV

```{r}
# plan(multiprocess)
# fits_tuning <- fit_cv_tuning(wflow_list_tuning, grid_list)
# fits_tuning
```


```{r}
# fits_no_tuning$fit %>% map(collect_metrics)
# fits_no_tuning$fit %>% map(collect_predictions)
```

```{r}
fits_tuning$fit %>% map(collect_metrics) %>%  map(arrange,.metric, mean)
```

```{r}
fits_tuning$fit %>% map(collect_predictions) 
```

```{r}
fits_tuning$fit %>% map(show_best, metric = "rmse", maximize = FALSE, n = 3)
```


# Refit by Best Params

## Update Models to Best Params

```{r}
best_param_list <-
  fits_tuning$fit %>% 
    map(show_best, metric = "rmse", maximize = FALSE, n = 1) %>% 
    map(select, -c(".metric", ".estimator", "mean", "n", "std_err"))

best_tune_lm <- tune_lm %>% update(best_param_list$lm)
best_tune_rf <- tune_rf %>% update(best_param_list$rf)
best_tune_mars <- tune_mars %>% update(best_param_list$mars)
best_tune_xgb <- tune_xgb %>% update(best_param_list$xgb)

```

## Prepare the Models (Refit)

```{r}
wflow_lm <- workflow() %>% add_recipe(rec) %>% add_model(best_tune_lm)
wflow_rf <- workflow() %>% add_recipe(rec) %>% add_model(best_tune_rf)
wflow_mars <- workflow() %>% add_recipe(rec) %>% add_model(best_tune_mars)
wflow_xgb <- workflow() %>% add_recipe(rec) %>% add_model(best_tune_xgb)
# wflow_keras <- workflow() %>% add_recipe(rec) %>% add_model(best_tune_keras)
```

## Create Wflow List  (best params)

```{r}
wflow_list_best <-
  list(lm = wflow_lm, rf = wflow_rf, mars = wflow_mars, xgb = wflow_xgb)
```

## Final Modeling by Best Params 

```{r}
fit_final_model <- function(wflow, data){
  fits <- fit(wflow, data = data)
  return(fits$fit)
}

plan(multiprocess)
fits_final <-
  wflow_list_best %>% 
    future_map(
      ~fit_final_model(
        wflow = .x,
        data = train
        )
      )
```

# Predict Test

```{r}
baked_test <- 
  test %>% 
  bake(rec_preped, new_data = .)

preds_list <- fits_final %>% map(~predict(object = .x$fit, select(baked_test, -target_var)))
preds_d <- preds_list %>% bind_cols() 
colnames(preds_d) <- paste0("pred_", names(preds_list))

baked_test <- baked_test %>% bind_cols(preds_d) 
baked_test
```

## plot
```{r}
colnames(preds_d) %>% map(my_test_confirm_plot)
```

# Final test (submit)


```{r}
saveRDS(fits_tuning, "fits_tuning.rds")
saveRDS(fits_best_params, "fits_best_params.rds")
```


